"""
LunarLander DQN with Gymnasium and TensorBoard Logging
Task 1: Reinforcement Learning Track
Author: [Your Name]
Date: 2024
"""

import gymnasium as gym
import numpy as np
import random
import os
import time
from collections import deque
from datetime import datetime

# TensorFlow 2.x with TensorBoard
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# ==================== HYPERPARAMETERS ====================
TRAINING = True  # Set to True for training, False for testing

# Learning parameters
LEARNING_RATE = 0.0005
DISCOUNT_FACTOR = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995

# Training parameters
LEARNING_EPISODES = 1000 if TRAINING else 0  # æ”¹ä¸º1000ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
TESTING_EPISODES = 100 if not TRAINING else 0
REPLAY_BUFFER_SIZE = 100000
REPLAY_BUFFER_BATCH_SIZE = 64
MINIMUM_REWARD = -250

# Environment parameters
STATE_SIZE = 8
NUMBER_OF_ACTIONS = 4

# File paths
WEIGHTS_FILENAME = './weights/lunar_lander_dqn.h5'
LOG_DIR = './logs/' + datetime.now().strftime("%Y%m%d-%H%M%S")

# ==================== DQN AGENT ====================
class DQNAgent:
    def __init__(self, training=True):
        self.training = training
        self.memory = deque(maxlen=REPLAY_BUFFER_SIZE)
        
        # Exploration parameters
        self.epsilon = EPSILON_START if training else 0.0
        self.epsilon_decay = EPSILON_DECAY
        self.epsilon_min = EPSILON_END
        
        # Create Q-network and target network
        self.model = self._build_q_network()
        self.target_model = self._build_q_network()
        self.target_model.set_weights(self.model.get_weights())
        
        # Optimizer and loss function
        self.optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
        self.loss_fn = keras.losses.Huber()
        
        # For TensorBoard logging - ä½¿ç”¨TensorFlowçš„SummaryWriter
        if training:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            os.makedirs(LOG_DIR, exist_ok=True)
            self.writer = tf.summary.create_file_writer(LOG_DIR)
            print(f"ğŸ“ TensorBoardæ—¥å¿—ç›®å½•å·²åˆ›å»º: {LOG_DIR}")
        else:
            self.writer = None
            
        self.episode_rewards = []
        self.episode_losses = []
        
        # Load weights if testing
        if not training:
            self.load_weights(WEIGHTS_FILENAME)
    
    def _build_q_network(self):
        """Build Deep Q-Network using Keras"""
        model = keras.Sequential([
            layers.Dense(64, activation='relu', input_shape=(STATE_SIZE,)),
            layers.Dense(64, activation='relu'),
            layers.Dense(64, activation='relu'),
            layers.Dense(NUMBER_OF_ACTIONS, activation='linear')
        ])
        return model
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        if self.training:
            self.memory.append((state, action, reward, next_state, done))
    
    def choose_action(self, state):
        """Epsilon-greedy action selection"""
        if not self.training or np.random.random() > self.epsilon:
            state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)
            q_values = self.model(state_tensor, training=False)
            return np.argmax(q_values[0].numpy())
        else:
            return np.random.randint(NUMBER_OF_ACTIONS)
    
    def train(self):
        """Train on a batch from replay buffer"""
        if len(self.memory) < REPLAY_BUFFER_BATCH_SIZE:
            return 0
        
        # Sample batch from memory
        batch = random.sample(self.memory, REPLAY_BUFFER_BATCH_SIZE)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # Convert to tensors
        states = tf.convert_to_tensor(states, dtype=tf.float32)
        actions = tf.convert_to_tensor(actions, dtype=tf.int32)
        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)
        dones = tf.convert_to_tensor(dones, dtype=tf.float32)
        
        # Compute target Q-values
        next_q_values = self.target_model(next_states, training=False)
        max_next_q = tf.reduce_max(next_q_values, axis=1)
        target_q = rewards + (1 - dones) * DISCOUNT_FACTOR * max_next_q
        
        # Compute current Q-values
        with tf.GradientTape() as tape:
            all_q_values = self.model(states, training=True)
            q_values = tf.reduce_sum(
                all_q_values * tf.one_hot(actions, NUMBER_OF_ACTIONS), 
                axis=1
            )
            loss = self.loss_fn(target_q, q_values)
        
        # Backpropagation
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
        
        return loss.numpy()
    
    def update_target_network(self):
        """Update target network weights"""
        self.target_model.set_weights(self.model.get_weights())
    
    def update_epsilon(self):
        """Decay exploration rate"""
        if self.training and self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_weights(self, filename):
        """Save model weights"""
        self.model.save_weights(filename)
        print(f"âœ… Model saved to {filename}")
    
    def load_weights(self, filename):
        """Load model weights"""
        if os.path.exists(filename):
            self.model.load_weights(filename)
            self.target_model.set_weights(self.model.get_weights())
            print(f"âœ… Model loaded from {filename}")
        else:
            print(f"âš ï¸  No weights found at {filename}, using random initialization")
    
    def log_metrics(self, episode, reward, loss, steps, epsilon):
        """Log metrics to TensorBoard - ä½¿ç”¨TensorFlowçš„API"""
        if self.writer:
            with self.writer.as_default():
                tf.summary.scalar('Reward/Episode', reward, step=episode)
                tf.summary.scalar('Loss/Episode', loss, step=episode)
                tf.summary.scalar('Steps/Episode', steps, step=episode)
                tf.summary.scalar('Epsilon', epsilon, step=episode)
                
                # Log average rewards every 10 episodes
                if len(self.episode_rewards) >= 10:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    tf.summary.scalar('Reward/Average_10', avg_reward, step=episode)
                
                if len(self.episode_losses) >= 10:
                    avg_loss = np.mean(self.episode_losses[-10:])
                    tf.summary.scalar('Loss/Average_10', avg_loss, step=episode)
                
                # åˆ·æ–°å†™å…¥å™¨ä»¥ç¡®ä¿æ•°æ®è¢«ä¿å­˜
                self.writer.flush()
    
    def close_writer(self):
        """Close TensorBoard writer"""
        if self.writer:
            self.writer.close()
            print("ğŸ“Š TensorBoard writer closed")

# ==================== TRAINING FUNCTION ====================
def train_agent():
    """Main training function"""
    print("=" * 60)
    print("ğŸš€ Starting LunarLander DQN Training")
    print("=" * 60)
    print(f"ğŸ“Š Hyperparameters:")
    print(f"   Learning Rate: {LEARNING_RATE}")
    print(f"   Discount Factor: {DISCOUNT_FACTOR}")
    print(f"   Epsilon Decay: {EPSILON_DECAY}")
    print(f"   Batch Size: {REPLAY_BUFFER_BATCH_SIZE}")
    print(f"   Max Episodes: {LEARNING_EPISODES}")
    print(f"   Log Directory: {LOG_DIR}")
    print("=" * 60)
    
    # æ£€æŸ¥æ—¥å¿—ç›®å½•
    print(f"ğŸ“ æ£€æŸ¥æ—¥å¿—ç›®å½•: {LOG_DIR}")
    print(f"ğŸ“ ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(LOG_DIR)}")
    
    # Create environment with Gymnasium
    env = gym.make(
        "LunarLander-v2",
        render_mode=None  # No rendering during training for speed
    )
    
    # Initialize agent
    agent = DQNAgent(training=True)
    
    # Track metrics
    all_rewards = []
    episode_lengths = []
    average_rewards = deque(maxlen=100)
    
    # Create weights directory if it doesn't exist
    os.makedirs('./weights', exist_ok=True)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # Training loop
    for episode in range(LEARNING_EPISODES):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0
        steps = 0
        
        for t in range(1000):  # Max steps per episode
            # Select and execute action
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Store experience
            agent.remember(state, action, reward, next_state, done)
            
            # Train on batch
            loss = agent.train()
            if loss > 0:
                episode_loss += loss
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            steps += 1
            
            if done or episode_reward < MINIMUM_REWARD:
                break
        
        # Update target network and epsilon
        agent.update_target_network()
        agent.update_epsilon()
        
        # Calculate average loss per step
        avg_loss_per_step = episode_loss / max(steps, 1)
        
        # Store metrics
        all_rewards.append(episode_reward)
        episode_lengths.append(steps)
        average_rewards.append(episode_reward)
        agent.episode_rewards.append(episode_reward)
        agent.episode_losses.append(avg_loss_per_step)
        
        # Log metrics to TensorBoard
        agent.log_metrics(
            episode, 
            episode_reward, 
            avg_loss_per_step, 
            steps, 
            agent.epsilon
        )
        
        # Print progress
        if episode % 10 == 0:
            avg_reward = np.mean(average_rewards) if average_rewards else 0
            elapsed_time = time.time() - start_time
            episodes_per_sec = (episode + 1) / elapsed_time if elapsed_time > 0 else 0
            
            print(f"ğŸ“ˆ Episode {episode:4d} | "
                  f"Reward: {episode_reward:7.2f} | "
                  f"Avg Reward: {avg_reward:7.2f} | "
                  f"Steps: {steps:4d} | "
                  f"Epsilon: {agent.epsilon:.4f} | "
                  f"Memory: {len(agent.memory):6d} | "
                  f"EPS: {episodes_per_sec:.2f}/s")
        
        # Save model every 100 episodes
        if episode % 100 == 0 and episode > 0:
            checkpoint_file = f"./weights/checkpoint_ep{episode}.h5"
            agent.save_weights(checkpoint_file)
            
            # æ£€æŸ¥TensorBoardäº‹ä»¶æ–‡ä»¶
            event_files = [f for f in os.listdir(LOG_DIR) if 'tfevents' in f]
            print(f"ğŸ“Š TensorBoardäº‹ä»¶æ–‡ä»¶: {len(event_files)} ä¸ªæ–‡ä»¶")
        
        # Early stopping if performance is good
        if len(average_rewards) == 100 and np.mean(average_rewards) > 200:
            print(f"ğŸ‰ Early stopping at episode {episode}: Average reward > 200!")
            break
    
    # Save final model
    agent.save_weights(WEIGHTS_FILENAME)
    
    # Close TensorBoard writer
    agent.close_writer()
    
    # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
    total_time = time.time() - start_time
    print(f"â±ï¸  Total training time: {total_time:.2f} seconds")
    
    env.close()
    
    # Plot training results
    plot_training_results(all_rewards, episode_lengths)
    
    # æ˜¾ç¤ºTensorBoardä½¿ç”¨è¯´æ˜
    print("\n" + "=" * 60)
    print("ğŸ“Š TENSORBOARD INSTRUCTIONS:")
    print("=" * 60)
    print("1. åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ:")
    print("   tensorboard --logdir logs/")
    print("2. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€:")
    print("   http://localhost:6006")
    print("3. æŸ¥çœ‹ä»¥ä¸‹æŒ‡æ ‡:")
    print("   - Reward/Episode: æ¯ä¸ªå›åˆçš„å¥–åŠ±")
    print("   - Loss/Episode: æ¯ä¸ªå›åˆçš„æŸå¤±")
    print("   - Steps/Episode: æ¯ä¸ªå›åˆçš„æ­¥æ•°")
    print("   - Epsilon: æ¢ç´¢ç‡è¡°å‡")
    print("   - Reward/Average_10: æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±")
    print("=" * 60)
    
    return all_rewards

# ==================== TESTING FUNCTION ====================
def test_agent():
    """Test the trained agent"""
    print("=" * 60)
    print("ğŸ§ª Testing LunarLander DQN Agent")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(WEIGHTS_FILENAME):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {WEIGHTS_FILENAME}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼ (TRAINING = True)")
        return []
    
    # Create environment with rendering
    env = gym.make(
        "LunarLander-v2",
        render_mode="human"  # Enable rendering for visualization
    )
    
    # Initialize agent
    agent = DQNAgent(training=False)
    
    # Track metrics
    test_rewards = []
    
    for episode in range(TESTING_EPISODES):
        state, _ = env.reset()
        episode_reward = 0
        steps = 0
        
        for t in range(1000):
            # Select action (no exploration during testing)
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            steps += 1
            
            if done or episode_reward < MINIMUM_REWARD:
                break
        
        test_rewards.append(episode_reward)
        print(f"ğŸ§ª Test Episode {episode:3d} | Reward: {episode_reward:7.2f} | Steps: {steps:4d}")
    
    env.close()
    
    # Print test statistics
    print("=" * 60)
    print("ğŸ“Š Test Results:")
    print(f"   Total Episodes: {len(test_rewards)}")
    print(f"   Average Reward: {np.mean(test_rewards):.2f}")
    print(f"   Std Deviation: {np.std(test_rewards):.2f}")
    print(f"   Minimum Reward: {np.min(test_rewards):.2f}")
    print(f"   Maximum Reward: {np.max(test_rewards):.2f}")
    
    # Calculate success rate
    success_count = sum(1 for r in test_rewards if r > 200)
    success_rate = (success_count / len(test_rewards)) * 100 if test_rewards else 0
    print(f"   Success Rate (>200): {success_rate:.1f}% ({success_count}/{len(test_rewards)})")
    print("=" * 60)
    
    return test_rewards

# ==================== VISUALIZATION ====================
def plot_training_results(rewards, steps):
    """Plot training results"""
    if not rewards:
        print("âš ï¸  No training data to plot")
        return
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # Plot rewards
    ax1.plot(rewards, alpha=0.6, linewidth=1)
    ax1.set_xlabel('Episode', fontsize=12)
    ax1.set_ylabel('Reward', fontsize=12)
    ax1.set_title('Training Rewards per Episode', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # Add moving average
    window_size = 50
    if len(rewards) >= window_size:
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        ax1.plot(range(window_size-1, len(rewards)), moving_avg, 'r-', linewidth=2, 
                label=f'{window_size}-episode moving average')
        ax1.legend(fontsize=10)
    
    # Add horizontal line for success threshold
    ax1.axhline(y=200, color='green', linestyle='--', alpha=0.5, label='Success threshold (200)')
    ax1.legend(fontsize=10)
    
    # Plot steps
    ax2.plot(steps, alpha=0.6, color='green', linewidth=1)
    ax2.set_xlabel('Episode', fontsize=12)
    ax2.set_ylabel('Steps', fontsize=12)
    ax2.set_title('Episode Length (Steps)', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save the figure
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'./training_results_{timestamp}.png'
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"ğŸ“ˆ Training plot saved to {filename}")
    
    # Show the plot (å¦‚æœè¿è¡Œåœ¨GUIç¯å¢ƒ)
    try:
        plt.show()
    except:
        print("ğŸ“Š Plot generated but not displayed (running in non-GUI environment)")

# ==================== VERIFY TENSORBOARD ====================
def verify_tensorboard_setup():
    """Verify TensorBoard setup"""
    print("ğŸ” Verifying TensorBoard setup...")
    
    # æ£€æŸ¥æ—¥å¿—ç›®å½•
    if not os.path.exists('./logs'):
        os.makedirs('./logs', exist_ok=True)
        print("âœ… Created logs directory")
    
    # åˆ›å»ºæµ‹è¯•äº‹ä»¶æ–‡ä»¶
    test_log_dir = './logs/test_' + datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs(test_log_dir, exist_ok=True)
    
    writer = tf.summary.create_file_writer(test_log_dir)
    
    # å†™å…¥æµ‹è¯•æ•°æ®
    with writer.as_default():
        for i in range(10):
            tf.summary.scalar('test_scalar', i * 1.5, step=i)
        writer.flush()
    
    writer.close()
    
    # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†äº‹ä»¶æ–‡ä»¶
    import glob
    event_files = glob.glob(f"{test_log_dir}/*.tfevents*")
    
    if event_files:
        print(f"âœ… TensorBoard setup verified: {len(event_files)} event file(s) created")
        print(f"ğŸ“ Test log directory: {test_log_dir}")
    else:
        print("âŒ TensorBoard setup failed: No event files created")
    
    return len(event_files) > 0

# ==================== MAIN ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸŒ™ LunarLander-v2 DQN Implementation")
    print("ğŸ“š Task 1: Reinforcement Learning Track")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    np.random.seed(42)
    tf.random.set_seed(42)
    random.seed(42)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('./weights', exist_ok=True)
    os.makedirs('./logs', exist_ok=True)
    
    # éªŒè¯TensorBoardè®¾ç½®
    tensorboard_ok = verify_tensorboard_setup()
    if not tensorboard_ok:
        print("âš ï¸  TensorBoard setup may have issues, but continuing anyway...")
    
    print("\n" + "=" * 60)
    
    if TRAINING:
        # è®­ç»ƒæ¨¡å¼
        print("ğŸ¯ MODE: TRAINING")
        print("=" * 60)
        rewards = train_agent()
        
        # è®­ç»ƒåå¿«é€Ÿæµ‹è¯•
        if rewards and len(rewards) > 0:
            print("\n" + "=" * 60)
            print("ğŸ” Quick Test after Training")
            print("=" * 60)
            
            # ä¸´æ—¶åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼
            original_training = TRAINING
            TRAINING = False
            
            try:
                test_agent()
            except Exception as e:
                print(f"âš ï¸  Quick test failed: {e}")
            
            TRAINING = original_training
    else:
        # æµ‹è¯•æ¨¡å¼
        print("ğŸ¯ MODE: TESTING")
        print("=" * 60)
        test_rewards = test_agent()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Program completed successfully!")
    print("=" * 60)